{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other relevant topic labelling\n",
    "\n",
    "**Context**\n",
    "We are monitoring drivers for the diffusion of AI research in different industries. In a first pass of the analysis we operationalise these drivers with an expanded keyword search. We briefly outlined the problems with this in `01a_jmg_`, namely, that our vocabualaries are small and we don't have an index of certainty in our classification. Here we address this by training a model on a labelled dataset including information about projects related to ethics and projects related to law. We will then use that model to predict the probability that any project in the data is considering ethical issues.\n",
    "\n",
    "**Activities**\n",
    "1. Load and process GtR data\n",
    "2. Identify relevant keywords\n",
    "3. Train and evaluate models\n",
    "4. Test results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_preamble.ipy\n",
    "%run lda_pipeline.py\n",
    "%run text_classifier.py\n",
    "%run keyword_searches.py\n",
    "%run utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_gtr_df = pd.read_csv(\n",
    "    '../data/raw/gtr/gtr_projects.csv',\n",
    "    converters={\n",
    "        'research_topics': ast.literal_eval,\n",
    "        'researc_subjects': ast.literal_eval,\n",
    "    }\n",
    ")\n",
    "\n",
    "gtr_df = raw_gtr_df[(raw_gtr_df['start_year'] >= 2006) & (raw_gtr_df['start_year'] < 2017)]\n",
    "\n",
    "gtr_df.dropna(axis=0,subset=['abstract_texts'],inplace=True)\n",
    "\n",
    "gtr_df = gtr_df.loc[[len(x)>0 for x in gtr_df['research_topics']]]\n",
    "\n",
    "#gtr_df = gtr_df[(gtr_df['funder_name'] != 'BBSRC') & (gtr_df['funder_name'] != 'MRC')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_ethical = [x for x in set(flatten_list(gtr_df['research_topics'])) if any(var in x.lower().split(' ') for var in ['law','rights','jurisprudence',\n",
    "                                                                                                                        'legal','ethics','ethical','moral','privacy'\n",
    "                                                                                                                       ])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_df['legal_ethical'] = [(len(set(topics)&set(legal_ethical))/len(topics))>0.5 for topics in gtr_df['research_topics']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.get_dummies(gtr_df['legal_ethical'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = TextClassification(corpus=gtr_df['abstract_texts'],target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run grid search with these model parameters\n",
    "models = [\n",
    "    [RandomForestClassifier(),\n",
    "     {'class_weight':['balanced'],'min_samples_leaf':[1,5]}],\n",
    "    \n",
    "    [LogisticRegression(),\n",
    "     {'class_weight':['balanced'],'penalty':['l1','l2'],\n",
    "      'C':[0.1,1,100]}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.grid_search(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check scores and best estimators\n",
    "for res in tc.results:\n",
    "    print(res.best_score_)\n",
    "    print(res.best_estimator_)\n",
    "    \n",
    "    #This is the best estimator\n",
    "best_est = tc.results[1].best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = OrangeBrick(true_labels=np.array(target),\n",
    "                      predicted_labels=best_est.predict(tc.X),\n",
    "                      var_names=target.columns).make_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=2,figsize=(10,7.5))\n",
    "\n",
    "diag.confusion_chart(ax=ax[0])\n",
    "diag.prec_rec_chart(ax=ax[1])\n",
    "\n",
    "#fig.suptitle('Model evaluation for GTR disciplines',y=1.01,size=16)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(best_est.predict(tc.X)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_comb = pd.concat([pd.DataFrame(target),pd.DataFrame(best_est.predict(tc.X)),gtr_df['abstract_texts']],axis=1)\n",
    "var_comb.columns = ['actual_no_legal','actual_legal','pred_no_legal','pred_legal','abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_check(var_comb.loc[(var_comb.actual_legal==1) & (var_comb.pred_legal==1)]['abstract'],length=1000,num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_check(var_comb.loc[(var_comb.actual_legal==0) & (var_comb.pred_legal==1)]['abstract'],length=1000,num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_check(var_comb.loc[(var_comb.actual_legal==1) & (var_comb.pred_legal==0)]['abstract'],length=1000,num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling approach 2: Using document vectors\n",
    "\n",
    "We will use document vectors (in 300 dimensional space) to predict the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the tokenised corpus\n",
    "corpus_tokenised = CleanTokenize(gtr_df['abstract_texts']).clean().bigram().tokenised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the tagged documents\n",
    "tagged_docs = [TaggedDocument(w,[i]) for i,w in enumerate(corpus_tokenised)]\n",
    "\n",
    "#Train the doc2vec model\n",
    "d2v = Doc2Vec(documents=tagged_docs,size=300,window=5,min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run grid search with these model parameters\n",
    "models = [\n",
    "    [RandomForestClassifier(),\n",
    "     {'class_weight':['balanced'],'min_samples_leaf':[1,5]}],\n",
    "    \n",
    "    [LogisticRegression(),\n",
    "     {'class_weight':['balanced'],'penalty':['l1','l2'],\n",
    "      'C':[0.1,1,100]}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(target,features,models):\n",
    "        '''\n",
    "        Grid search over models with different parameters. \n",
    "        \n",
    "        Arguments:\n",
    "            target: the variable(s) we want to predict\n",
    "            features: the predictor\n",
    "            models: dicts with parameters we will grid search over\n",
    "            \n",
    "        returns:\n",
    "            The results of the grid search\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load inputs and targets into the model\n",
    "        Y = target\n",
    "        X = features\n",
    "        \n",
    "        for mod in models:\n",
    "            #Make ovr\n",
    "            mod[0] = OneVsRestClassifier(mod[0])\n",
    "                \n",
    "            #Add the estimator prefix\n",
    "            mod[1] = {'estimator__'+k:v for k,v in mod[1].items()}\n",
    "        \n",
    "        #Container with results\n",
    "        results = []\n",
    "\n",
    "        #For each model, run the analysis.\n",
    "        for num,mod in enumerate(models):\n",
    "            print(num)\n",
    "\n",
    "            #Run the classifier\n",
    "            clf = GridSearchCV(mod[0],mod[1])\n",
    "\n",
    "            #Fit\n",
    "            clf.fit(X,Y)\n",
    "\n",
    "            #Append results\n",
    "            results.append(clf)\n",
    "        \n",
    "        return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_features = np.array(d2v.docvecs.vectors_docs)\n",
    "\n",
    "doc_models = grid_search(target=target,features=doc2vec_features,models=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check scores and best estimators\n",
    "for res in doc_models:\n",
    "    print(res.best_score_)\n",
    "    print(res.best_estimator_)\n",
    "    \n",
    "    #This is the best estimator\n",
    "best_est = doc_models[1].best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eth_diag = OrangeBrick(true_labels=np.array(target),\n",
    "                      predicted_labels=best_est.predict(doc2vec_features),\n",
    "                      var_names=target.columns).make_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=2,figsize=(10,7.5))\n",
    "\n",
    "eth_diag.confusion_chart(ax=ax[0])\n",
    "eth_diag.prec_rec_chart(ax=ax[1])\n",
    "\n",
    "#fig.suptitle('Model evaluation for GTR disciplines',y=1.01,size=16)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
