{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI predictive analysis using MAG data\n",
    "\n",
    "\n",
    "**Background**\n",
    "\n",
    "The first step towards monitoring the diffusion of AI in various research fields is identifying AI. `01_jmg_` uses a keyword expansion strategy to identify projects that mention terms related to AI. This approach has a couple of limitations: \n",
    "\n",
    "1. it uses a relatively small number of terms, with the risk of low recall (we miss projects that don't mention our relatively limited vocabulary). \n",
    "2. (and relatedly) the low freqency across the corpus makes it hard to create scores capturing our confidence about results, which could lower our precision.\n",
    "\n",
    "**Goals**\n",
    "\n",
    "Here, we use an alternative strategy to identify AI projects using a labelled dataset from Microsoft Academic Graph that we have obtained for another project. The idea is to train a model that predicts which of the papers in the dataset have AI as a label, using the words in the abstract as features. We will then transfer that model to the GtR projects and compare results with the keyword based approach.\n",
    "\n",
    "**Activities**\n",
    "1. Load, process and briefly explore the MAG data\n",
    "2. Train model\n",
    "3. Evaluate model\n",
    "4. Load GtR data and compare model performances\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_preamble.ipy\n",
    "%run lda_pipeline.py\n",
    "%run text_classifier.py\n",
    "%run keyword_searches.py\n",
    "%run utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other imports\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put functions here\n",
    "\n",
    "def random_check(corpus,num,length):\n",
    "    '''\n",
    "    Prints num random examples form corpus\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    selected = np.random.randint(0,len(corpus),num)\n",
    "    \n",
    "    texts  = [text for num,text in enumerate(corpus) if num in selected]\n",
    "    \n",
    "    for t in texts:\n",
    "        print(t[:length])\n",
    "        print('====')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_data = []\n",
    "\n",
    "mag_path = '../data/raw/mag_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = os.listdir(mag_path)\n",
    "\n",
    "#For each year (folder in the directory) we extract items and put them in the mag_data list\n",
    "for y in years:\n",
    "    \n",
    "    print(y)\n",
    "    \n",
    "    dir_in_y = os.listdir(mag_path+f'/{y}')\n",
    "    \n",
    "    for item in dir_in_y:\n",
    "        with open(mag_path+f'/{y}/{item}','r') as infile:\n",
    "            file = json.load(infile)\n",
    "            mag_data.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_flat = flatten_list(mag_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_flat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks to work with the above:\n",
    "\n",
    "* Extract the labels ('F')\n",
    "* Reconstruct the abstract ('E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the MAG data\n",
    "\n",
    "I want to keep the topics, the years and the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mag(mag_object):\n",
    "    '''\n",
    "    Parses the mag data\n",
    "    \n",
    "    Arguments:\n",
    "        -mag_object: a dict with various fields of interest. We want to extract the year, the topics and reconstruct an inverted index\n",
    "        (this is the way that microsoft stores its abstract data)\n",
    "    \n",
    "    Returns:\n",
    "        -A dict with the three element\n",
    "    \n",
    "    Observations:\n",
    "        -If there is a failure we store a missing value.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    my_id = mag_object['Id']\n",
    "    \n",
    "    try:\n",
    "        topic = [x['FN'] for x in mag_object['F']]\n",
    "        year = int(mag_object['D'].split('-')[0])\n",
    "        ia = literal_eval(mag_object['E'])['IA']['InvertedIndex']\n",
    "            #This goes through the ia and reorders the words. Note that we are removing stopwords as we go\n",
    "        a = [it[0] for it in sorted(flatten_list([[(k.lower(),n) for n in v if k.lower() not in stop] for k,v in ia.items()]),key = lambda x: x[1])]\n",
    "\n",
    "        return({'topic':topic,'year':y,'abstract':a})\n",
    "    \n",
    "    except:\n",
    "        return(f'failed item {my_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_parsed = []\n",
    "\n",
    "for n,i in enumerate(mag_flat):\n",
    "    \n",
    "    if n%5000==0:\n",
    "        print(n)\n",
    "    \n",
    "    \n",
    "    mag_parsed.append(parse_mag(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_clean = [x for x in mag_parsed if type(x)!=str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mag_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have lost quite a few papers that are missing their abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(flatten_list([x['topic'] for x in mag_clean])).value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*6400/len(mag_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6000 papers with AI (3.6% of the total). Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling approach 1: text classifier\n",
    "\n",
    "This approach is quite crude and probably will take a long time (we have a big corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We choose a random sample of a quarter of the data to speed things up\n",
    "chosen = np.random.randint(0,high=len(mag_clean),\n",
    "                  size=int(len(mag_clean)/1.5))\n",
    "\n",
    "mag_chosen = [x for num,x in enumerate(mag_clean) if num in chosen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.get_dummies(pd.Series([1 if 'artificial intelligence' in x['topic'] else 0 for x in mag_chosen]))\n",
    "\n",
    "#Turn this tokenised set into strings for additional pre-processing\n",
    "corpus = [' '.join(x['abstract']) for x in mag_chosen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = TextClassification(corpus=corpus,target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run grid search with these model parameters\n",
    "models = [\n",
    "    [RandomForestClassifier(),\n",
    "     {'class_weight':['balanced'],'min_samples_leaf':[1,5]}],\n",
    "    \n",
    "    [LogisticRegression(),\n",
    "     {'class_weight':['balanced'],'penalty':['l1','l2'],\n",
    "      'C':[0.1,1,100]}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.grid_search(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check scores and best estimators\n",
    "for res in tc.results:\n",
    "    print(res.best_score_)\n",
    "    print(res.best_estimator_)\n",
    "    \n",
    "    #This is the best estimator\n",
    "best_est = tc.results[1].best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_diag = OrangeBrick(true_labels=np.array(target),\n",
    "                      predicted_labels=best_est.predict(tc.X),\n",
    "                      var_names=target.columns).make_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=2,figsize=(10,7.5))\n",
    "\n",
    "ai_diag.confusion_chart(ax=ax[0])\n",
    "ai_diag.prec_rec_chart(ax=ax[1])\n",
    "\n",
    "#fig.suptitle('Model evaluation for GTR disciplines',y=1.01,size=16)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(best_est.predict_proba(tc.X)>0.5).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent model performance! Let's check some of the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_comb = pd.concat([pd.DataFrame(target),pd.DataFrame(best_est.predict(tc.X)),pd.Series(corpus)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_comb.columns = ['actual_no_ai','actual_ai','pred_no_ai','pred_ai','abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_check(ai_comb.loc[(ai_comb.actual_ai==1) & (ai_comb.pred_ai==1)]['abstract'],length=1000,num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_check(ai_comb.loc[(ai_comb.actual_ai==0) & (ai_comb.pred_ai==1)]['abstract'],length=1000,num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False negatives seem to be quantitative papers with no AI activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_check(ai_comb.loc[(ai_comb.actual_ai==1) & (ai_comb.pred_ai==0)]['abstract'],length=1000,num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling approach application b. Predict ethical / legal vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling approach 2: Using document vectors\n",
    "\n",
    "We will use document vectors (in 300 dimensional space) to predict the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the tokenised corpus\n",
    "\n",
    "corpus_tokenised = CleanTokenize([' '.join(x['abstract']) for x in mag_clean]).clean().bigram().tokenised\n",
    "\n",
    "#Create the tagged documents\n",
    "tagged_docs = [TaggedDocument(w,[i]) for i,w in enumerate(corpus_tokenised)]\n",
    "\n",
    "#Train the doc2vec model\n",
    "d2v = Doc2Vec(documents=tagged_docs,size=350,window=10,min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run grid search with these model parameters\n",
    "models = [\n",
    "    [RandomForestClassifier(),\n",
    "     {'class_weight':['balanced'],'min_samples_leaf':[1,5]}],\n",
    "    \n",
    "    [LogisticRegression(),\n",
    "     {'class_weight':['balanced'],'penalty':['l1','l2'],\n",
    "      'C':[0.1,1,100]}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(target,features,models):\n",
    "        '''\n",
    "        Grid search over models with different parameters. \n",
    "        \n",
    "        Arguments:\n",
    "            target: the variable(s) we want to predict\n",
    "            features: the predictor\n",
    "            models: dicts with parameters we will grid search over\n",
    "            \n",
    "        returns:\n",
    "            The results of the grid search\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load inputs and targets into the model\n",
    "        Y = target\n",
    "        X = features\n",
    "        \n",
    "        for mod in models:\n",
    "            #Make ovr\n",
    "            mod[0] = OneVsRestClassifier(mod[0])\n",
    "                \n",
    "            #Add the estimator prefix\n",
    "            mod[1] = {'estimator__'+k:v for k,v in mod[1].items()}\n",
    "        \n",
    "        #Container with results\n",
    "        results = []\n",
    "\n",
    "        #For each model, run the analysis.\n",
    "        for num,mod in enumerate(models):\n",
    "            print(num)\n",
    "\n",
    "            #Run the classifier\n",
    "            clf = GridSearchCV(mod[0],mod[1])\n",
    "\n",
    "            #Fit\n",
    "            clf.fit(X,Y)\n",
    "\n",
    "            #Append results\n",
    "            results.append(clf)\n",
    "        \n",
    "        return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.get_dummies(pd.Series([1 if 'artificial intelligence' in x['topic'] else 0 for x in mag_clean]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_features = np.array(d2v.docvecs.vectors_docs)\n",
    "\n",
    "doc_models = grid_search(target=target,features=doc2vec_features,models=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check scores and best estimators\n",
    "for res in doc_models:\n",
    "    print(res.best_score_)\n",
    "    print(res.best_estimator_)\n",
    "    \n",
    "    #This is the best estimator\n",
    "best_est = doc_models[1].best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_doc2vec = OrangeBrick(true_labels=np.array(target),\n",
    "                      predicted_labels=best_est.predict(doc2vec_features),\n",
    "                      var_names=target.columns).make_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=2,figsize=(10,7.5))\n",
    "\n",
    "ai_doc2vec.confusion_chart(ax=ax[0])\n",
    "ai_doc2vec.prec_rec_chart(ax=ax[1])\n",
    "\n",
    "#fig.suptitle('Model evaluation for GTR disciplines',y=1.01,size=16)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(best_est.predict(doc2vec_features)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the GtR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr = pd.read_csv('../data/interim/04-04-2019_projects_all_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_vect = tc.count_vect.transform(gtr['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = best_est.predict_proba(gtr_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr['is_ai_model'] = pd.DataFrame(out)[1]\n",
    "gtr['is_ai_model_bin'] = gtr['is_ai_model'].apply(lambda x: x>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(gtr['is_ai_model_bin'],gtr.has_ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_check(gtr.loc[(gtr['is_ai_model_bin']==1) & (gtr.has_ai==1)]['abstract'],10,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_check(gtr.loc[(gtr['is_ai_model_bin']==0) & (gtr.has_ai==1)]['abstract'],10,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_check(gtr.loc[(gtr['is_ai_model_bin']==1) & (gtr.has_ai==0)]['abstract'],10,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results comparison\n",
    "\n",
    "Both classification strategies produce quite different results. Let's explore both to decide how to classify projects into AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr['classification'] = ['both' if (x==True) & (y==True) else 'no_ai' if (x==False) & (y==False) else 'only_kw' if (x==True) & (y==False) else 'only_model'\n",
    "                         for x,y in zip(gtr['has_ai'],gtr['is_ai_model_bin'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_ai = gtr.loc[gtr['classification']!='no_ai']\n",
    "\n",
    "disc_counts = pd.concat([gtr_ai.loc[gtr['top_disc']==disc]['classification'].value_counts(normalize=True) for disc in [var for var in gtr_ai.columns if 'disc_' in var]],axis=1)\n",
    "\n",
    "#gtr[['has_ai','is_ai_model_bin']].sum()\n",
    "\n",
    "disc_counts.columns = [var for var in gtr_ai.columns if 'disc_' in var]\n",
    "\n",
    "disc_counts.T.plot.bar(stacked=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(gtr_ai['year'],gtr_ai['classification']).rolling(window=3).mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the differences between the vocabularies?\n",
    "\n",
    "1. Calculate word freqs for all docs\n",
    "2. Calculate word freqs for all ai docs\n",
    "3. Calculate normalised word freqs for ai classes vs all and vs all ai (written as a function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_tokenised = CleanTokenize(gtr['abstract']).clean().bigram()\n",
    "gtr_tokens = gtr_tokenised.tokenised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_freqs = pd.Series(flatten_list(gtr_tokens)).value_counts()\n",
    "\n",
    "all_freqs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr['has_ai_either'] = gtr[['has_ai','is_ai_model_bin']].sum(axis=1)>0\n",
    "\n",
    "ai_indices = [n for n,e in enumerate(gtr['has_ai_either']) if e==True]\n",
    "\n",
    "ai_freqs = pd.Series(flatten_list([x for n,x in enumerate(gtr_tokens) if n in ai_indices])).value_counts()\n",
    "\n",
    "ai_freqs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that returns frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_salient_words(corpus,normaliser,threshold):\n",
    "    '''\n",
    "    Function to normalise word frequencies in a corpus by another and return top ones (controlling for frequency to remove noise)\n",
    "    \n",
    "    Arguments\n",
    "        corpus: the tokenised corpus where we are looking for salient terms\n",
    "        normaliser: the word frequencies to normalise with\n",
    "        threshold: number of times a word has to appear in the original corpus for being included\n",
    "        return = number of words to return (defaults to all)\n",
    "    \n",
    "    Return\n",
    "        A df with the total number of appearances of a word and its normalised frequency.  \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    freqs = pd.Series(flatten_list(corpus)).value_counts()\n",
    "    \n",
    "    freqs_selected = freqs.loc[freqs>threshold]\n",
    "    \n",
    "    #Calculate frequencies\n",
    "    to_norm = pd.concat([freqs_selected,normaliser],axis=1).apply(lambda x: x/x.sum())\n",
    "    \n",
    "    to_norm.columns = ['in_corpus','for_norm']\n",
    "    \n",
    "    #Calculate normalised frequency\n",
    "    to_norm['freq_normalised'] = to_norm['in_corpus']/to_norm['for_norm']\n",
    "    \n",
    "    return(to_norm).dropna(axis=0).sort_values('freq_normalised',ascending=False)\n",
    "#     if returns=='all':\n",
    "#         return(to_norm.sort_values('freq_normalised',ascending=False))\n",
    "    \n",
    "#     else:\n",
    "#         return(to_norm.sort_values('freq_normalised',ascending=False).iloc[:returns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ai_corpus = [x for n,x in enumerate(gtr_tokens) if n in ai_indices]\n",
    "\n",
    "salient_all_ai = get_salient_words(all_ai_corpus,all_freqs,threshold=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_both,ai_model,ai_kw = [[w for i,w in enumerate(gtr_tokens) if i in indices] \n",
    "                           for indices in [[n for n,e in enumerate(gtr['classification']) if e ==val] for val in \n",
    "                           ['both','only_kw','only_model']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = []\n",
    "\n",
    "for indices,term in zip([ai_both,ai_model,ai_kw],['both','model','kw']):\n",
    "    \n",
    "    salient_all = get_salient_words(indices,all_freqs,threshold=40)\n",
    "    salient_ai = get_salient_words(indices,ai_freqs,threshold=40)\n",
    "    \n",
    "    salient_all.columns,salient_ai.columns = [[x+f'_{term}' for x in df.columns] for term,df in zip(['all','ai'],[salient_all,salient_ai])]\n",
    "    \n",
    "    \n",
    "    out = pd.concat([salient_all,salient_ai],axis=1)\n",
    "    \n",
    "    out['category'] = term\n",
    "    #out['pos'] = np.arange(0,len(out))\n",
    "    \n",
    "    processed.append(out)\n",
    "    \n",
    "extracted_freqs = pd.concat(processed,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ['both','model','kw']:\n",
    "    \n",
    "    print(x)\n",
    "    print('===')\n",
    "    \n",
    "    rel = extracted_freqs.loc[extracted_freqs['category']==x]\n",
    "\n",
    "    print('all_norm')\n",
    "    print(rel.sort_values('freq_normalised_all',ascending=False)[:10].index)\n",
    "\n",
    "    print('-------')\n",
    "    print('ai_norm')\n",
    "    print(rel.sort_values('freq_normalised_ai',ascending=False)[:10].index)\n",
    "\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gtr['classification']=np.nan\n",
    "\n",
    "for i,df in enumerate(gtr):\n",
    "    \n",
    "    if (gtr.loc[i,'has_ai']== 1 & gtr.loc[i,'is_ai_model_bin']==1):\n",
    "        gtr.loc[i,'classification']='bothx'\n",
    "        \n",
    "    elif (gtr.loc[i,'has_ai']== 1 & gtr.loc[i,'is_ai_model_bin']==0):\n",
    "        gtr.loc[i,'classification']='only_kw'\n",
    "    \n",
    "    elif (gtr.loc[i,'has_ai']== 0 & gtr.loc[i,'is_ai_model_bin']==1):\n",
    "        gtr.loc[i,'classification']='only_model'\n",
    "    \n",
    "    else:\n",
    "        gtr.loc[i,'classification']='none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr[['has_ai','is_ai_model_bin']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr[['has_ai','is_ai_model_bin','has_prediction','has_ethics','has_db','has_data']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
