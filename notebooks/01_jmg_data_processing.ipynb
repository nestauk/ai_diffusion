{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of GtR data for diffusion of AI paper\n",
    "\n",
    "**Draft pre-abstract**\n",
    "\n",
    "This paper analyses the diffusion of methods and technologies related to AI across various UK research fields using an open dataset about research funding covering the period 2006-2018. We are particularly interested in understanding differential rates of diffusion in research projects with different 'industrial orientations' (which we predict using a machine learning model trained on a large corpus of business descriptions) before and after 2012, a landmark year in the development of AI research, as well as the drivers of diffusion. Here, we focus on three potential explanations of AI diffusion:\n",
    "\n",
    "* The 'data intensity' of a field, which we estimate using the propensity of related research fields to generate data outputs\n",
    "* The 'prediction intensity' of a field, which we measure through the use of terms related to prediction, uncertainty and risk in its project descriptions\n",
    "* The 'ethical and legal' risks of a field, which we measure through the semantic similarity between projects in the field and academic research on ethical risks from AI.\n",
    "\n",
    "**Activities**\n",
    "1. Load the gateway to research data\n",
    "2. Identify AI papers.\n",
    "  * Approach a. Use a keyword based approach\n",
    "  * Approach b (robustness)\n",
    "    * Train a model on GTR data\n",
    "    * Use topic modelling\n",
    "3. Identify data-intensive sectors.\n",
    "  * Approach a. Use the data-productivity of different sectors\n",
    "  * Approach b. Use project semantic similarity to documents about data, data processing and infrastructure etc.\n",
    "4. Identify prediction-intensive sectors\n",
    "  * Approach a. Calculate semantic similarity between projects and documents describing relevant concepts such as prediction, risk, uncertainty, decision-making\n",
    "  * Approach b. Measure the 'prediction intensity' of different sectors based on their occupational distribution\n",
    "5. Identify ethical and legal issues.\n",
    "  * Approach a. Calculate the semantic similarity between projects in the domain and projects related to ethics and legal issues\n",
    "  * Approach b. Use social media data... somehow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preamble\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_preamble.ipy\n",
    "%run lda_pipeline.py\n",
    "%run text_classifier.py\n",
    "%run keyword_searches.py\n",
    "%run utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put functions and classes here\n",
    "\n",
    "def flatten_list(a_list):\n",
    "    return([x for el in a_list for x in el])\n",
    "\n",
    "\n",
    "def random_check(corpus,num,length):\n",
    "    '''\n",
    "    Prints num random examples form corpus\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    selected = np.random.randint(0,len(corpus),num)\n",
    "    \n",
    "    texts  = [text for num,text in enumerate(corpus) if num in selected]\n",
    "    \n",
    "    for t in texts:\n",
    "        print(t[:length])\n",
    "        print('====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_data(corpus_to_label,corpus_tokenised,w2v,seed_list,threshold,name,occ_threshold):\n",
    "    '''\n",
    "    This function queries a word2vec model to identify synonyms for an initial seed vocabulary, finds words with that vocabulary in the data,\n",
    "    and labels a df with them.\n",
    "    \n",
    "    Arguments\n",
    "    \n",
    "        -corpus_to_label: a df where every row is a document. We want to label them\n",
    "        -corpus_tokenised: a bag of words od elements corresponding to the documents\n",
    "        -w2v: word2vec model used for the expansion\n",
    "        -seed_lis: the list of terms we want to expand\n",
    "        -threshold: similarity threshold when expanding the keyword\n",
    "        -name to label the relevant documents\n",
    "        \n",
    "    Returns\n",
    "         \n",
    "        A list with the final set of keywords used for labelling, and a labelled df (labels include number of occurrences of words in the expanded seed and\n",
    "        a boolean indicating if the word occurs or not)\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Initialise the keywordExpansion object\n",
    "    kw_exp = keywordExpander(corpus_to_label,corpus_tokenised,w2v)\n",
    "    kw_exp.keyword_expansion(seed_list,thres=threshold)\n",
    "    \n",
    "    labeller = keywordLabeller(kw_exp)\n",
    "    labeller.label_data(name=name)\n",
    "\n",
    "    labelled_output = labeller.projects_labelled\n",
    "    labelled_output['has_'+name] = labelled_output[name]>occ_threshold\n",
    "    \n",
    "    \n",
    "    out = [kw_exp.expanded_keywords,labelled_output]\n",
    "    \n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_str = datetime.datetime.strftime(datetime.datetime.today(),'%d-%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_latest_file\n",
    "import datetime\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't want to print all the info logs\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [x for x in os.listdir('../data/raw/') if 'labelled' in x]\n",
    "\n",
    "latest_file = get_latest_file(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr = pd.read_csv('../data/raw/'+latest_file,index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to focus on research grants since 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a couple of column sets\n",
    "\n",
    "discs, outputs = [[x for x in gtr.columns if w in x] for w in ['disc_','out_']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Focus on grants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grants = gtr.query('grant_category == \"Research Grant\" & year > 2006 & year < 2019').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grants.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33k grant awards with abstracts since 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grants['top_disc'] = grants[discs].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grants.top_disc.value_counts().plot.bar(color='blue',title='Discipline distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,10),nrows=2)\n",
    "\n",
    "pd.crosstab(grants.year,grants.top_disc).rolling(window=3).mean().plot(ax=ax[0],title='Discipline funding evolution')\n",
    "\n",
    "pd.pivot_table(\n",
    "    grants.groupby(['year','top_disc'])['amount'].sum().reset_index(drop=False),\n",
    "    index='year',columns='top_disc',aggfunc='sum').rolling(window=3).mean().plot(ax=ax[1],legend=False)\n",
    "\n",
    "ax[0].legend(bbox_to_anchor=(1,0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Find AI projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_seed = ['machine_learning','artificial_intelligence','deep_learning','ai','machine_vision','text_mining','data_mining']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create sentence corpus\n",
    "sentence_corpus = flatten_list([x.split('. ') for x in grants['abstract']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize etc using the classes above\n",
    "sentence_tokenised = CleanTokenize(sentence_corpus).clean().bigram()\n",
    "\n",
    "#Also tokenise by documents so we can query them later\n",
    "corpus_tokenised = CleanTokenize(grants['abstract']).clean().bigram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training W2V\n",
    "w2v = Word2Vec(sentence_tokenised.tokenised,window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_thres=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_labelling_outputs = label_data(grants,corpus_tokenised.tokenised,w2v,ai_seed,0.85,'ai',occ_threshold=occ_thres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the keyword classes and functions in `keyword_searches` above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the labelled df from the outputs\n",
    "grants_labelled_ai = ai_labelling_outputs[1]\n",
    "\n",
    "grants_labelled_ai['has_ai'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 1600 'AI' projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_check(grants_labelled_ai.loc[grants_labelled_ai['ai']>0]['abstract'],10,length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelled_data_plots(labelled_df,var_name,\n",
    "                        ax,\n",
    "                        cross_tab_against=['top_disc','funder'],\n",
    "                        do_random_check=True,**kwargs):\n",
    "    '''\n",
    "    \n",
    "    Produces some plots of the data focusing on the labelled observations\n",
    "\n",
    "    Arguments\n",
    "    \n",
    "        -labelled_df: the labelled dataset\n",
    "        -var_name: name of the variable to report on\n",
    "        -cross_tabs_against: variables to crosstab against\n",
    "        -ax: matplotlib axis object\n",
    "        -do_random_check: print a random sample of labelled data (using the kwargs to manage outputs)\n",
    "\n",
    "    Returns\n",
    "    \n",
    "        -Line chart of activity as share of the total (number of projects and total funding)\n",
    "        -For each crosstab:\n",
    "            -Barchart with distribution of projects and funding\n",
    "            -Linechart with project activity trends\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    df = labelled_df\n",
    "\n",
    "    has_var = f'has_{var_name}'\n",
    "        \n",
    "    is_df = df.loc[df[has_var]==True]\n",
    "    \n",
    "\n",
    "    \n",
    "    print(f'{var_name} has {len(is_df)} projects ({np.round(100*(len(is_df)/len(df)),2)}% of the total)')\n",
    "    print(f'It has received Â£{is_df.amount.sum()}, ({np.round(100*is_df.amount.sum()/df.amount.sum(),2)}% of the total)')\n",
    "    \n",
    "    #Example projects\n",
    "    \n",
    "    if do_random_check==True:\n",
    "        print('\\n')\n",
    "        print(f'{var_name} EXAMPLES')\n",
    "        print('=======')\n",
    "        random_check(df.loc[df[has_var]>0]['abstract'],kwargs['number'],kwargs['length'])\n",
    "        \n",
    "    #Plots\n",
    "    \n",
    "    #Linechart\n",
    "        #Projects\n",
    "    \n",
    "    #The first mean below is calculating the share of a boolean. The second is for the rolling mean\n",
    "    df.groupby('year')[has_var].mean().rolling(window=2).mean().plot(title=f'{var_name} activity as a share of the total',ax=ax[0])\n",
    "    \n",
    "        #Funding\n",
    "    \n",
    "    fund_y = pd.pivot_table(df.groupby(['year',has_var])['amount'].sum().reset_index(drop=False),\n",
    "                   index='year',columns=has_var,values='amount')\n",
    "    fund_share = fund_y[True]/(fund_y[False]+fund_y[True])\n",
    "    \n",
    "    fund_share.rolling(window=2).mean().plot(ax=ax[0])\n",
    "    \n",
    "    ax[0].legend(labels=['projects','funding'])\n",
    "    \n",
    "    \n",
    "    #Shares\n",
    "    \n",
    "    for num,var in enumerate(cross_tab_against):\n",
    "        df.groupby(var)[has_var].mean().sort_values(ascending=False).plot.bar(color='blue',ax=ax[1+num],title=f'Share of {var} in {has_var}')\n",
    "        \n",
    "    #Trends\n",
    "\n",
    "    \n",
    "    for num,var in enumerate(cross_tab_against):\n",
    "        pd.crosstab(is_df['year'],is_df[var],normalize=1).rolling(window=3).mean().plot(\n",
    "            ax=ax[3+num],title=f'Share of {var_name} activity by {var} / year')\n",
    "\n",
    "        ax[3+num].legend(bbox_to_anchor=(1,1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some preliminary explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AI Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=5,figsize=(10,20))\n",
    "\n",
    "labelled_data_plots(grants_labelled_ai,'ai',ax=ax,**{'number':5,'length':1000,'thres':0})\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify prediction intensity\n",
    "\n",
    "We will use the same approach as above but focusing on terms related to prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_seed = ['prediction', 'uncertainty', 'risk', 'decision', 'probability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB label the AI labelled set\n",
    "pred_labelling_outputs = label_data(grants_labelled_ai,corpus_tokenised.tokenised,w2v,pred_seed,0.7,'prediction',occ_threshold=occ_thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labelling_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=5,figsize=(10,20))\n",
    "\n",
    "labelled_data_plots(pred_labelling_outputs[1],'prediction',ax=ax,**{'number':5,'length':1000})\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify projects related to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_seed = ['data','dataset','data_sets']\n",
    "\n",
    "labelled_df = pred_labelling_outputs[1]\n",
    "\n",
    "#NB label the AI labelled set\n",
    "data_labelling_outputs = label_data(labelled_df,corpus_tokenised.tokenised,w2v,data_seed,0.7,'data',occ_threshold=occ_thres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labelling_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=5,figsize=(10,20))\n",
    "\n",
    "labelled_data_plots(data_labelling_outputs[1],'data',ax=ax,**{'number':5,'length':1000})\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify projects related to ethics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethical_seed = ['legal','ethical','ethics','privacy','tort']\n",
    "\n",
    "labelled_df = data_labelling_outputs[1]\n",
    "\n",
    "#NB label the AI labelled set\n",
    "ethical_labelling_outputs = label_data(labelled_df,corpus_tokenised.tokenised,w2v,ethical_seed,0.8,'ethics',occ_threshold=occ_thres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethical_labelling_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=5,figsize=(10,20))\n",
    "\n",
    "labelled_data_plots(ethical_labelling_outputs[1],'ethics',ax=ax,**{'number':5,'length':1000})\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify projects related "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_df = ethical_labelling_outputs[1]\n",
    "\n",
    "labelled_df['has_db']= labelled_df['out_db']>0\n",
    "\n",
    "labelled_df[['has_ai','has_prediction','has_data','has_ethics','has_db']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_check(labelled_df.loc[(labelled_df['has_ai']==True)&(labelled_df['has_ethics']==True)]['abstract'],num=30,length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_df.to_csv(f'../data/interim/{today_str}_projects_all_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6*5*750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
